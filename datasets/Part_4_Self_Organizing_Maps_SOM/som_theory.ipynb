{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapas Autorganizados (SOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se toma un conjunto de datos multidimensional los cuales se pueden mostrar como un mapa bidimensional. El objetivo del **SOM** es reducir las columnas y quedarnos solo con las que nos aporten más info. O lo que es lo mismo, reducir lo máximo la dimensión de la entrada de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_2.jpg\" heigh=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el mapa **SOM** central nos indica la prosperidad o pobreza de los paises agrupados en colores, usando como inputs varios datos de éstos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo aprenden los SOM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abajo podemos ver 3 columnas de datos de entrada ($X_{1},X_{2},X_{3}$) convertido en un **SOM** en 2-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_3.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abajo tenemos la misma red neuronal pero puesto los nodos de salida en una sola columna. Cada Nodo le llegan 3 sinapsis-enlace de los nodos de entrada, cada nodo tiene un peso asociado para cada enlace $W_{i,j}$.\n",
    "\n",
    "En las redes neurnales artificiales los pesos se usaban para multiplicar la entrada del peso de ese nodo para luego sumarlo, agruparlo o lo que fuese en la neurona final y aplicarle una función de activación, en cambio en los mapas autorganizados **SOM** no hay función de activación, los pesos son una característica del propio nodo y por lo tanto dichos pesos no se colocan en las sinápsis sino en el propio nodo (ahora los nodos tienen una serie de coordenadas dados por los pesos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_4.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es descubrir el nodo que más cercano está a cada uno de los nodos de entrada, para ello calculamos su distancia (por ejmplo la Euclidea). En el caso de arriba observamos que el nodo3 es el más cercano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_6.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_5.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si pasamos los nodos a un plano bidimensional y si nos imaginamos que el nodo verde (figura **1**) es el nodo más cercano a las variables input $X_j$, lo que pasará después es que el **SOM** actualizará los pesos para que cada uno de ellos distribuya a las observaciones.\n",
    "\n",
    "Si nos fijamos en el dibujo inferior, lo que vemos en términos visuales descrito arriba es que el punto más cercano \"estirará\" del mapa hasta él para que se acerque todavía más en las siguientes iteraciones. En definitiva, para cada uno de los nodos, va a intentar cambiar las coordenadas(pesos) de dicho nodos para que cada vez se parezcan más a las observaciones.\n",
    "\n",
    "A continuación lo que se hará será tomar del punto en cuestión(nodo verde en **1**) un radio que hará que los nodos que se encuentren dentro de esta zona amarilla se actualizarán sus coordenadas-pesos, siendo los nodos más cercanos al verde los que más se actualicen y viceversa.\n",
    "\n",
    "Luego se procederá igual, por ejemplo con el nodo azúl que se parezca más a las variables de entrada (ver **2**), con su propio radio de actuación y hará lo mismo. En el momento que un nodo entra en conflicto con el azúl o el verde, obviamente será atraído más por el nodo más cercano (nos fijamos en la figura **3**). Si nos fijamos en la figura **4** vemos que para un nodo intermedio entre el azúl y verde, al estar en medio tenría un color intermedio, el turquesa. Y para los que están cerca del verde tendrían un colo casi verde del todo, con muy poco de azúl.\n",
    "\n",
    "Obteniendo así finalmente un mapa de colores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_7.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/som_8.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos más unidades de mayor coincidencia con las variables de entrada pasará lo mismo obteniendo el mapa de color.\n",
    "\n",
    "En cada iteración (epoch) los radios de acción amarillos se irán haciendo más pequeños. EL proceso se vuelve más y más preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importante recordar:\n",
    "- Los SOMs retienen la topología del conjunto de entrada.\n",
    "- Los SOMs revelan correlaciones que no se identifican fácilmente.\n",
    "- Los SOMs clasifican datos sin supervisión alguna.\n",
    "- No hay vector de variable target -> no hay propagación hacia atrás.\n",
    "- No hay conexiones laterales entre los nodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ejemplo de SOM](http://ai-junkie.com/ann/som/som1.html) con programa descargable *.exe*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo leer un mapa autorganizado SOM avanzado:\n",
    "\n",
    "Ejemplo de SOM de patrones en el voto de 535 senadores nortaméricanos\n",
    "<img src=\"../img/som_9.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos para entrenar un SOM:\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 1:</span> Empezamos con un dataset compuesto de *n_features* variables independientes.\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 2:</span> Preparamos una parrilla compuesta de nodos, cada uno con un vector de pesos de *n_features* elementos.\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 3:</span> Aleatoriamente inicializamos valores del vector de pesos a números pequeños cercanos a $0$ (pero no $0$).\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 4:</span> Seleccionar una observación aleatoria del dataser.\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 5:</span> Calcular la distancia Euclídea desde dicho puntos a las diferentes neuronas de la red.\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 6:</span> Seleccionar la neurona con la menor distancia al punto. Dicha neurona es el nodo ganador.\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 7:</span> Actualizar lso epsos del nodo ganador para moverlo más cerca dle punto.\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 8:</span> Utilizar una función Gaussiana al vecindario del punto de medie el nodo ganador y actualizar los pesos de los vecinos para moverlos más cerca del punto. El radio de los vecinos afectados es la desviación típica de la Gaussiana.\n",
    "\n",
    "- <span style='color:#288c17'> <b>PASO 9:</span> Repetir los pasos <span style='color:#288c17'> <b>1</span> a <span style='color:#288c17'> <b>5</span> y actualizar los pesos después de cada observación (*Reinforcement Learning*) o después de un conjunto de observaciones (*Batch Learning*), hasta que la red neuronal converja en un punto donde los vecindarios no cambien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
