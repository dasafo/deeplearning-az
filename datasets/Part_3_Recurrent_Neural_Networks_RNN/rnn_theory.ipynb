{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes Neuronales Recurrentes** (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/rnr_1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colores del esquema simplificado de una red neuronal *(como si la vieramos desde arriba)*:\n",
    "\n",
    "    amarillo--> Vector de entrada\n",
    "    azúl--> Capas ocultas (cada circulo representa más capas ocultas)\n",
    "    rojo--> vector de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ocasiones las capas ocultas volverán sobre si mismas en cada iteración, de ahí lo de **Recurrentes**.\n",
    "\n",
    "## Ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/rnr_2.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **One to Many:** Tenemos solo un valor posible de entrada y muchos de salida posibles. Un ejemplo es cuando suminstramos una imágen a una red neuronal que nos describe lo que hay.\n",
    " \n",
    "**Many to One:** Tenemos varios inputs y una sola salida, como podía ser el análisis de texto que nos dijese si ese tecto es algo positivo o negativo.\n",
    "\n",
    "**Many to Many:** Muchos inputs de entrada y outputs de salida. Como por ejemplo está el google translator. O de los subtitulos a las películas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recordemos que el **gradiente descendente** intentaba buscar el mínimo global de la *función de costes* con el fin de encontrar una solución óptima. Recordemos también que en una red neuronal, la información pasa hacia adelante (capas ocultas y finalmente su salida) usando la técnica del gradiente descendente, la función de pérdidas(costes) generaba el error y se propagaba hacia atrás en la red neurnal actualizando en este proceso los pesos gracias a este gradiente descendente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/vanishing_1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que ocurre en las Redes Neuronales Recurrentes es algo similar pero con mayor complejidad, ya que en estas la información puede volver a la misma neurona de la capa oculta *(recordar que en el dibujo de arriba cada circulo morado de izquierda a derecha, es la misma capa de neuronas ocultas pero trasladadas en el tiempo)*. Cuando calculamos la función de costes o de error, en nuestro caso tenemos una posible salida para cada uno de nuestros nodos temporales (cirulos rojos), lo que nos dará una serie de errores en cada tiempo *$\\normalsize\\textbf(\\varepsilon_{t-i})$*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/vanishing_2.jpg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver lo que pasa en este proceso, centrándonos en una sola salida con su error de coste $\\normalsize\\textbf(\\varepsilon_{t})$. Ahora si queremos valancear este error, aplicando el gradiente descendente hacia atrás, ya no solo son responsables los pesos de las neuronas ocultas que están justo debajo *$\\normalsize\\textbf(X_{t})$*, sino también las que temporalmente también están atrás $\\normalsize\\textbf(X_{t-1},X_{t-2}, ...)$. Por lo tanto la correción de error irá hacia atrás en el espacio y el tiempo *(ver flechas moradas)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según vemos en la ecuación **(5)** el peso recurrente $\\normalsize\\textbf W_{rec}$ se irá multiplicando hacia atrás siendo este valor cada vez más pequeño *(ver flechas verdes)*, ya que cuando inicializamos una red neuronal solemos poner valores a los pesos cercanos a 0, que estos conforme vamos avanzando van cambiando. Esto hace que los pesos estarán mal corregidos por lo tanto la salida también, por eso se llama el **problema del desvanecimiento del gradiente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soluciones al problema del desvanecimiento del gradiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Exploding Gradient (si $\\normalsize\\textbf W_{rec}>1$)\n",
    "\n",
    "- Propagación hacia atrás truncada\n",
    "- Penalizaciones\n",
    "- Gradient Clipping\n",
    "\n",
    "\n",
    "**2.** Vanishing Gradient (si $\\normalsize\\textbf W_{rec}<1$)\n",
    "\n",
    "- Inicialización de Pesos\n",
    "- Echo State Networks\n",
    "- Long Short-Term Memory Networks (LSTMs) <span style='color:#4185af'> <b><----------</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nos vamos a centrar en el [**L**ong **S**hort-**T**erm **M**emory Network**s**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/lstm_1.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vemos una red neruonal recurrente como hemos visto arriba, siendo la capa verde la capa oculta. Al pasar por la $tanh$ tendríamos el problema del desvanecimiento o Vanishing (cada vez tendriamos menos gradiente y se magnificaría el este efecto).\n",
    "\n",
    "Esta es la solución que propone el algoritmo **LSTMs** dentro de esa capa oculta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/lstm_2.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La flecha horizontal superior se refiere a un peso igual a la unidad ($W_{rec}=1$) donde tenemos un producto y una suma. La cual irá variando dependiento del flujo de info que le llegue por abajo.\n",
    "- Las $C_{t-1}$ es la info que entra de la celda  adyacente(la capa oculta antes). Y y $C_{t}$ la info que lleva nuestra celda a la posterior en el tiempo.\n",
    "- Las $h_{t,t-1}$ es como antes pero para las salidas.\n",
    "- Las \"válvulas\" $\\otimes$ de las operaciones puntuales. \n",
    "    - La de la izquierda se llama ***forget valve***. Controla el flujo de la info que se conserva frente a la nueva que le entra por abajo.\n",
    "    - La del centro ***memory valve***. Al igual que la de la izquierda, también controla la info que le llega a la linea negra de arriba, esta vez aplicando una $tanh$ (función sigmoide $0$ o $1$).\n",
    "    - La de la izquierda ***output valve***\n",
    "- Las cajas amrillas representan toda una red neuronal con sus funciones de activación $\\sigma$ o $tanh$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos que sigue esta **LSTMs**:\n",
    "\n",
    "**1.** Entra una capa de información $\\textbf X_{t}$ que se junta con la capa de información $\\textbf h_{t-1}$ de la fase anterior de la capa de la red neuronal recurrente. Esta info se combina \n",
    "y con la función *sigma* ($0,1$) nos indica si la válvula de arriba estará abierta($1$) o cerrada($0$).\n",
    "\n",
    "**2.** La segunda fase es también la combinación de $\\textbf X_{t}$ y $\\textbf h_{t-1}$. Que llega a dos operaciones. \n",
    "- Una *sigma* que nos dirá si esa válvula central(*memory valve*) estará abierta o no.\n",
    "- Una función $tanh$ con la info que fluirá si está abierta hacia arriba.\n",
    "\n",
    "**3.** Se combina otra vez $\\textbf X_{t}$ y $\\textbf h_{t-1}$ para abrir una válvula que nos indicará si la info que viene de la línea superior se combina o no con la $tanh$ y de ahí pasa a la $h_t$ inferior, cuya info pasará a la siguiente etapa de la capa oculta por un lado, y por otro será la salida de esta capa oculta en esta etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Idea práctica](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) de LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
