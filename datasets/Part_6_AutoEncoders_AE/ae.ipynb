{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset\n",
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users  = pd.read_csv(\"ml-1m/users.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings  = pd.read_csv(\"ml-1m/ratings.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar el conjunto de entrenamiento y elconjunto de testing\n",
    "training_set = pd.read_csv(\"ml-100k/u1.base\", sep = \"\\t\", header = None) #base=trainig\n",
    "training_set = np.array(training_set, dtype = \"int\") #para pytorch pasamos el dataframe de pandas a array\n",
    "test_set = pd.read_csv(\"ml-100k/u1.test\", sep = \"\\t\", header = None) #test=test\n",
    "test_set = np.array(test_set, dtype = \"int\") #pasamos el dataframe a formato array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el número de usuarios y de películas\n",
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datos en un array X[u,i] con usuarios 'u' en fila y películas 'i' en columna\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_user in range(1, nb_users+1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies-1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datos a tensores de Torch\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la arquitectura de la Red Neuronal\n",
    "class SAE(nn.Module): #clase que herada de otra clase Module, siendo 'nn' el paquete de Pytorch donde se encuentra\n",
    "    def __init__(self, ): #constructor de la clase\n",
    "        super(SAE, self).__init__() #llamaos al constructor del padre usando super(...).__init__()\n",
    "        \n",
    "        #deinifmos los AutoEncoders apilados que consistira en codificar los datos de entrada\n",
    "        #pasar de 20 neuronas a 10 (codificacion) y de 10 a 20 otra vez(decodificacion):\n",
    "        \n",
    "        #full connection(fc), capa de entrada con nb_movies numero de neuronas y \n",
    "        #con capa oculta 20 caracteristicas-neuronas \n",
    "        self.fc1 = nn.Linear(nb_movies, 20) \n",
    "        #Otra fc capa oculta que conectara la capa anterior (que tenia 20) y le ponemos una salida\n",
    "        #de 10 neuronas de salida (codificacion)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        #lo mismo de arriba que conecta las 10 neuronas de arriba con 20 en otra capa oculta que ponemos\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        #lo mismo, conectamos las 20 neuronas de salida de fc3 a la capa de salida con nb_movies-neuronas\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        \n",
    "        #funcion de activacion que indicara el umbral para que se activen las neuronas\n",
    "        self.activation = nn.Sigmoid() \n",
    "    \n",
    "    #funcion donde la info fluira desde la codificacion a la descodificacion\n",
    "    #self para usar la variables del contructor de la clase y 'x' indicara los datos\n",
    "    def forward(self, x): \n",
    "        x = self.activation(self.fc1(x)) #codificacion pasado por la funcion de activacion\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x) #queremos el valor de la pelicula, ya no hace falta meter la funcion de activacion\n",
    "        return x\n",
    "\n",
    "sae = SAE() #creamos un objeto de la clase SAE()\n",
    "criterion = nn.MSELoss() #criterio de optimizacion(error) usando el error cuadrado medio (MSE) para que el entrenamiento suceda\n",
    "#optimizador usando RMS(tambien se podría usar el Adam)\n",
    "#lr=learning rate(tasa de aprendizaje)\n",
    "#weight_decay para ajustar el lr\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: tensor(1.7712)\n",
      "Epoch: 2, Loss: tensor(1.0967)\n",
      "Epoch: 3, Loss: tensor(1.0534)\n",
      "Epoch: 4, Loss: tensor(1.0382)\n",
      "Epoch: 5, Loss: tensor(1.0310)\n",
      "Epoch: 6, Loss: tensor(1.0265)\n",
      "Epoch: 7, Loss: tensor(1.0238)\n",
      "Epoch: 8, Loss: tensor(1.0219)\n",
      "Epoch: 9, Loss: tensor(1.0209)\n",
      "Epoch: 10, Loss: tensor(1.0195)\n",
      "Epoch: 11, Loss: tensor(1.0190)\n",
      "Epoch: 12, Loss: tensor(1.0185)\n",
      "Epoch: 13, Loss: tensor(1.0180)\n",
      "Epoch: 14, Loss: tensor(1.0174)\n",
      "Epoch: 15, Loss: tensor(1.0174)\n",
      "Epoch: 16, Loss: tensor(1.0168)\n",
      "Epoch: 17, Loss: tensor(1.0170)\n",
      "Epoch: 18, Loss: tensor(1.0166)\n",
      "Epoch: 19, Loss: tensor(1.0165)\n",
      "Epoch: 20, Loss: tensor(1.0160)\n",
      "Epoch: 21, Loss: tensor(1.0160)\n",
      "Epoch: 22, Loss: tensor(1.0159)\n",
      "Epoch: 23, Loss: tensor(1.0159)\n",
      "Epoch: 24, Loss: tensor(1.0159)\n",
      "Epoch: 25, Loss: tensor(1.0159)\n",
      "Epoch: 26, Loss: tensor(1.0155)\n",
      "Epoch: 27, Loss: tensor(1.0154)\n",
      "Epoch: 28, Loss: tensor(1.0152)\n",
      "Epoch: 29, Loss: tensor(1.0130)\n",
      "Epoch: 30, Loss: tensor(1.0120)\n",
      "Epoch: 31, Loss: tensor(1.0097)\n",
      "Epoch: 32, Loss: tensor(1.0074)\n",
      "Epoch: 33, Loss: tensor(1.0068)\n",
      "Epoch: 34, Loss: tensor(1.0024)\n",
      "Epoch: 35, Loss: tensor(1.0031)\n",
      "Epoch: 36, Loss: tensor(1.0007)\n",
      "Epoch: 37, Loss: tensor(0.9993)\n",
      "Epoch: 38, Loss: tensor(0.9952)\n",
      "Epoch: 39, Loss: tensor(0.9933)\n",
      "Epoch: 40, Loss: tensor(0.9905)\n",
      "Epoch: 41, Loss: tensor(0.9912)\n",
      "Epoch: 42, Loss: tensor(0.9894)\n",
      "Epoch: 43, Loss: tensor(0.9877)\n",
      "Epoch: 44, Loss: tensor(0.9833)\n",
      "Epoch: 45, Loss: tensor(0.9828)\n",
      "Epoch: 46, Loss: tensor(0.9787)\n",
      "Epoch: 47, Loss: tensor(0.9785)\n",
      "Epoch: 48, Loss: tensor(0.9755)\n",
      "Epoch: 49, Loss: tensor(0.9744)\n",
      "Epoch: 50, Loss: tensor(0.9740)\n",
      "Epoch: 51, Loss: tensor(0.9770)\n",
      "Epoch: 52, Loss: tensor(0.9729)\n",
      "Epoch: 53, Loss: tensor(0.9660)\n",
      "Epoch: 54, Loss: tensor(0.9629)\n",
      "Epoch: 55, Loss: tensor(0.9620)\n",
      "Epoch: 56, Loss: tensor(0.9609)\n",
      "Epoch: 57, Loss: tensor(0.9625)\n",
      "Epoch: 58, Loss: tensor(0.9588)\n",
      "Epoch: 59, Loss: tensor(0.9609)\n",
      "Epoch: 60, Loss: tensor(0.9569)\n",
      "Epoch: 61, Loss: tensor(0.9559)\n",
      "Epoch: 62, Loss: tensor(0.9528)\n",
      "Epoch: 63, Loss: tensor(0.9540)\n",
      "Epoch: 64, Loss: tensor(0.9508)\n",
      "Epoch: 65, Loss: tensor(0.9498)\n",
      "Epoch: 66, Loss: tensor(0.9484)\n",
      "Epoch: 67, Loss: tensor(0.9504)\n",
      "Epoch: 68, Loss: tensor(0.9488)\n",
      "Epoch: 69, Loss: tensor(0.9480)\n",
      "Epoch: 70, Loss: tensor(0.9461)\n",
      "Epoch: 71, Loss: tensor(0.9466)\n",
      "Epoch: 72, Loss: tensor(0.9435)\n",
      "Epoch: 73, Loss: tensor(0.9448)\n",
      "Epoch: 74, Loss: tensor(0.9423)\n",
      "Epoch: 75, Loss: tensor(0.9433)\n",
      "Epoch: 76, Loss: tensor(0.9408)\n",
      "Epoch: 77, Loss: tensor(0.9418)\n",
      "Epoch: 78, Loss: tensor(0.9395)\n",
      "Epoch: 79, Loss: tensor(0.9406)\n",
      "Epoch: 80, Loss: tensor(0.9383)\n",
      "Epoch: 81, Loss: tensor(0.9396)\n",
      "Epoch: 82, Loss: tensor(0.9377)\n",
      "Epoch: 83, Loss: tensor(0.9382)\n",
      "Epoch: 84, Loss: tensor(0.9364)\n",
      "Epoch: 85, Loss: tensor(0.9377)\n",
      "Epoch: 86, Loss: tensor(0.9360)\n",
      "Epoch: 87, Loss: tensor(0.9366)\n",
      "Epoch: 88, Loss: tensor(0.9351)\n",
      "Epoch: 89, Loss: tensor(0.9358)\n",
      "Epoch: 90, Loss: tensor(0.9344)\n",
      "Epoch: 91, Loss: tensor(0.9351)\n",
      "Epoch: 92, Loss: tensor(0.9337)\n",
      "Epoch: 93, Loss: tensor(0.9345)\n",
      "Epoch: 94, Loss: tensor(0.9332)\n",
      "Epoch: 95, Loss: tensor(0.9337)\n",
      "Epoch: 96, Loss: tensor(0.9323)\n",
      "Epoch: 97, Loss: tensor(0.9326)\n",
      "Epoch: 98, Loss: tensor(0.9318)\n",
      "Epoch: 99, Loss: tensor(0.9323)\n",
      "Epoch: 100, Loss: tensor(0.9311)\n",
      "Epoch: 101, Loss: tensor(0.9319)\n",
      "Epoch: 102, Loss: tensor(0.9308)\n",
      "Epoch: 103, Loss: tensor(0.9312)\n",
      "Epoch: 104, Loss: tensor(0.9305)\n",
      "Epoch: 105, Loss: tensor(0.9309)\n",
      "Epoch: 106, Loss: tensor(0.9297)\n",
      "Epoch: 107, Loss: tensor(0.9302)\n",
      "Epoch: 108, Loss: tensor(0.9292)\n",
      "Epoch: 109, Loss: tensor(0.9297)\n",
      "Epoch: 110, Loss: tensor(0.9285)\n",
      "Epoch: 111, Loss: tensor(0.9293)\n",
      "Epoch: 112, Loss: tensor(0.9279)\n",
      "Epoch: 113, Loss: tensor(0.9286)\n",
      "Epoch: 114, Loss: tensor(0.9274)\n",
      "Epoch: 115, Loss: tensor(0.9280)\n",
      "Epoch: 116, Loss: tensor(0.9270)\n",
      "Epoch: 117, Loss: tensor(0.9276)\n",
      "Epoch: 118, Loss: tensor(0.9265)\n",
      "Epoch: 119, Loss: tensor(0.9269)\n",
      "Epoch: 120, Loss: tensor(0.9259)\n",
      "Epoch: 121, Loss: tensor(0.9264)\n",
      "Epoch: 122, Loss: tensor(0.9255)\n",
      "Epoch: 123, Loss: tensor(0.9258)\n",
      "Epoch: 124, Loss: tensor(0.9251)\n",
      "Epoch: 125, Loss: tensor(0.9253)\n",
      "Epoch: 126, Loss: tensor(0.9244)\n",
      "Epoch: 127, Loss: tensor(0.9248)\n",
      "Epoch: 128, Loss: tensor(0.9239)\n",
      "Epoch: 129, Loss: tensor(0.9242)\n",
      "Epoch: 130, Loss: tensor(0.9234)\n",
      "Epoch: 131, Loss: tensor(0.9238)\n",
      "Epoch: 132, Loss: tensor(0.9229)\n",
      "Epoch: 133, Loss: tensor(0.9231)\n",
      "Epoch: 134, Loss: tensor(0.9223)\n",
      "Epoch: 135, Loss: tensor(0.9226)\n",
      "Epoch: 136, Loss: tensor(0.9222)\n",
      "Epoch: 137, Loss: tensor(0.9223)\n",
      "Epoch: 138, Loss: tensor(0.9215)\n",
      "Epoch: 139, Loss: tensor(0.9218)\n",
      "Epoch: 140, Loss: tensor(0.9212)\n",
      "Epoch: 141, Loss: tensor(0.9213)\n",
      "Epoch: 142, Loss: tensor(0.9207)\n",
      "Epoch: 143, Loss: tensor(0.9210)\n",
      "Epoch: 144, Loss: tensor(0.9205)\n",
      "Epoch: 145, Loss: tensor(0.9204)\n",
      "Epoch: 146, Loss: tensor(0.9198)\n",
      "Epoch: 147, Loss: tensor(0.9200)\n",
      "Epoch: 148, Loss: tensor(0.9195)\n",
      "Epoch: 149, Loss: tensor(0.9195)\n",
      "Epoch: 150, Loss: tensor(0.9191)\n",
      "Epoch: 151, Loss: tensor(0.9193)\n",
      "Epoch: 152, Loss: tensor(0.9187)\n",
      "Epoch: 153, Loss: tensor(0.9189)\n",
      "Epoch: 154, Loss: tensor(0.9183)\n",
      "Epoch: 155, Loss: tensor(0.9183)\n",
      "Epoch: 156, Loss: tensor(0.9180)\n",
      "Epoch: 157, Loss: tensor(0.9181)\n",
      "Epoch: 158, Loss: tensor(0.9177)\n",
      "Epoch: 159, Loss: tensor(0.9175)\n",
      "Epoch: 160, Loss: tensor(0.9173)\n",
      "Epoch: 161, Loss: tensor(0.9173)\n",
      "Epoch: 162, Loss: tensor(0.9170)\n",
      "Epoch: 163, Loss: tensor(0.9169)\n",
      "Epoch: 164, Loss: tensor(0.9167)\n",
      "Epoch: 165, Loss: tensor(0.9165)\n",
      "Epoch: 166, Loss: tensor(0.9163)\n",
      "Epoch: 167, Loss: tensor(0.9162)\n",
      "Epoch: 168, Loss: tensor(0.9159)\n",
      "Epoch: 169, Loss: tensor(0.9159)\n",
      "Epoch: 170, Loss: tensor(0.9155)\n",
      "Epoch: 171, Loss: tensor(0.9156)\n",
      "Epoch: 172, Loss: tensor(0.9152)\n",
      "Epoch: 173, Loss: tensor(0.9150)\n",
      "Epoch: 174, Loss: tensor(0.9150)\n",
      "Epoch: 175, Loss: tensor(0.9150)\n",
      "Epoch: 176, Loss: tensor(0.9146)\n",
      "Epoch: 177, Loss: tensor(0.9145)\n",
      "Epoch: 178, Loss: tensor(0.9145)\n",
      "Epoch: 179, Loss: tensor(0.9145)\n",
      "Epoch: 180, Loss: tensor(0.9141)\n",
      "Epoch: 181, Loss: tensor(0.9141)\n",
      "Epoch: 182, Loss: tensor(0.9138)\n",
      "Epoch: 183, Loss: tensor(0.9138)\n",
      "Epoch: 184, Loss: tensor(0.9136)\n",
      "Epoch: 185, Loss: tensor(0.9135)\n",
      "Epoch: 186, Loss: tensor(0.9134)\n",
      "Epoch: 187, Loss: tensor(0.9133)\n",
      "Epoch: 188, Loss: tensor(0.9131)\n",
      "Epoch: 189, Loss: tensor(0.9131)\n",
      "Epoch: 190, Loss: tensor(0.9128)\n",
      "Epoch: 191, Loss: tensor(0.9129)\n",
      "Epoch: 192, Loss: tensor(0.9126)\n",
      "Epoch: 193, Loss: tensor(0.9127)\n",
      "Epoch: 194, Loss: tensor(0.9124)\n",
      "Epoch: 195, Loss: tensor(0.9122)\n",
      "Epoch: 196, Loss: tensor(0.9123)\n",
      "Epoch: 197, Loss: tensor(0.9121)\n",
      "Epoch: 198, Loss: tensor(0.9120)\n",
      "Epoch: 199, Loss: tensor(0.9119)\n",
      "Epoch: 200, Loss: tensor(0.9119)\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch+1): #bucle para recorrer cada la red 200 veces\n",
    "    \n",
    "    train_loss = 0 #perdidas para medir la cantidad de errores en el entrenamiento(usando mean)\n",
    "    s = 0. #Cuantos usuarios calificaron al menos una pelicula, en decimal\n",
    "    \n",
    "    for id_user in range(nb_users): #bucle para recorrer los usuarios\n",
    "        \n",
    "        #vector de entrada(valoraciones de cada usuario), al dar un vector de un elemento...\n",
    "        #pero pythorch o keras exige introducir un array con mas de un elemento(variable().unsqueeze(0))\n",
    "        #añadimos una dimesion 'falsa'\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0) \n",
    "        \n",
    "        #hacemos una copia de input(el vector de entrada para poder medir al final el error en funcion de la prevision\n",
    "        target = input.clone() \n",
    "        if torch.sum(target.data > 0) > 0: #para calificar usuarios que hayan calificado alguna pelicula (0 seria ninguna)\n",
    "            \n",
    "            #salida de la red para los pronosticos pasando por las capas creadas antes en SAE()\n",
    "            output = sae.forward(input)\n",
    "            #para ignorar el gradiente descendiente\n",
    "            target.require_grad = False\n",
    "            #solo queremos aplicar el gradiente descenente el target a predecir es 0\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10) \n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print(\"Epoch: \"+str(epoch)+\", Loss: \"+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: tensor(1.8734)\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el conjunto de test en nuestro SAE\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae.forward(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10) \n",
    "        test_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n",
    "        s += 1.\n",
    "print(\"Test Loss: \"+str(train_loss/s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
