{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders (AE)\n",
    "\n",
    "Como su nombre indica consistirá en codificar valores. Tomará unos valores de entrada, los codificará, pasará por los nodos ocultos, de descodificarán y saldrán por los nodos de salida exactamente con los mismos valores que tenían a la entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_2.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $1$ si a la persona le ha gustado la pelicula\n",
    "- $0$ si no le ha gustado la pelicula\n",
    "\n",
    "La idea es guardar 4 valoraciones-neuronas con solo 2 numeros-neuronas. Al final se aplica una función *Softmax* que convierte el valor mas grande en un $1$ y el resto en $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias (Término independiente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_3.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces los **Auto Encoders** también se pueden representar como arriba. Que se muestra también nodos con información adicional que se suministse a la función de activación *z* como sería la *b* que se añade ahí.\n",
    "\n",
    "Se puede representar en la capa de salida, o dentro de la capa oculta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo entrenar un Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estructura básica de un **Auto Encoder**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_4.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_5.jpg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_6.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobrecompletar Capas Ocultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_7.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tuvieramos más nodos en la capa oculta que en la de entrada y salida (si queremos extraer más características en vez de reducirlas), el *Auto Econder* podría falsear nuestas salidas ya que podría enviar en linea recta la información de un lado a otro si hacer nada, sería absurdo. Para resolver esto tenemos los **Sparse Autoencoders**, **Denoising Autoencoders**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_8.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica una técnica de regularización dónde se aplica una técnica de *sparseidad*, que sirve para regular el exceso de nodos. Si se envíainfo de la capa de entrada, no todas las neuronas de la oculta podrán actuar, introducimos una penalización para que no todos los nodos puedan estar activos *(en gris en el dibujo)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografía:\n",
    "- [Sparse Autoencoder](http://mccormickml.com/2014/05/30/deep-learning-tutorial-sparse-autoencoder/)\n",
    "- [k-Sparse Autoencoders](https://arxiv.org/pdf/1312.5663.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_9.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es otra técnica de regularización que consiste en desplazar la capa de entrada a la izquierda y se haría *\"una copia\"* donde algunas de ellas aleatoriamente se convierten en $0$. Después de la fase de entrenameiento (rojo) se comprarán con las de entrada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografía:\n",
    "\n",
    "[Extracting and Composing Robust Features with Denoising\n",
    "Autoencoders](http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contractive Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es otra técnica de regularización que consiste en ir comprarando la entrada y salida penalizando en la función de pérdidas en cada iteración, hasta que coincide la entrada con la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_10.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agraga otra capa oculta sobre la otra, ahora tendríamos dos capas ocultas de codificación. Es un algortimo muy poderso por encima que las *Máquinas de Boltzmann*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografía:\n",
    "\n",
    "[Stacked Denoising Autoencoders: Learning Useful Representations in\n",
    "a Deep Network with a Local Denoising Criterion](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/ae_11.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es como el apilado(stacked). Son máquinas de Boltzmann apiladas con algo más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografía:\n",
    "\n",
    "[Deep Autoencoders](https://www.cs.toronto.edu/~hinton/science.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
